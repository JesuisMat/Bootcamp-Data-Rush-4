# ============================================
# SCRIPT 3 : STANDARDISATION DES DONNÃ‰ES
# Fichier : 03_standardization.py
# ============================================

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib
import os

print("="*60)
print("ğŸ¯ STANDARDISATION DES DONNÃ‰ES POUR PCA ET K-MEANS")
print("="*60)

# ============================================
# 1. CHARGEMENT DES DONNÃ‰ES ENRICHIES
# ============================================
print("\nğŸ“‚ Chargement des donnÃ©es enrichies...")
df = pd.read_csv("output/enriched_marketing_data.csv", sep=';')
print(f"âœ… Dataset chargÃ© : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")

# ============================================
# 2. SÃ‰LECTION DES FEATURES POUR LE CLUSTERING
# ============================================
print("\n" + "="*60)
print("ğŸ“‹ SÃ‰LECTION DES FEATURES")
print("="*60)

clustering_features = [
    'Income', 
    'Age', 
    'Total_Spend', 
    'Days_Since_Last_Purchase', 
    'Days_Since_Join', 
    'Total_Purchases', 
    'Avg_Purchase_Value', 
    'Spend_Income_Ratio', 
    'NumWebVisitsMonth', 
    'Web_Preference', 
    'Deal_Seeker', 
    'Total_Children', 
    'family_size', 
    'is_parent', 
    'is_couple', 
    'education2_encoded'
]

print(f"\nâœ… {len(clustering_features)} features sÃ©lectionnÃ©es pour le clustering")
print("\nListe des features :")
for i, feat in enumerate(clustering_features, 1):
    print(f"   {i:2d}. {feat}")

# VÃ©rifier que toutes les colonnes existent
missing_cols = [col for col in clustering_features if col not in df.columns]
if missing_cols:
    print(f"\nâŒ ERREUR : Colonnes manquantes dans le dataset : {missing_cols}")
    exit(1)

# CrÃ©er le subset
X = df[clustering_features].copy()
print(f"\nğŸ“Š Dimensions du subset : {X.shape}")

# VÃ©rifier les valeurs manquantes
missing_values = X.isnull().sum().sum()
if missing_values > 0:
    print(f"\nâš ï¸ Attention : {missing_values} valeurs manquantes dÃ©tectÃ©es")
    print(X.isnull().sum()[X.isnull().sum() > 0])
    print("Suppression des lignes avec valeurs manquantes...")
    X = X.dropna()
    print(f"âœ… Nouvelles dimensions : {X.shape}")

# ============================================
# 3. STANDARDISATION
# ============================================
print("\n" + "="*60)
print("ğŸ”§ STANDARDISATION")
print("="*60)

print("\nğŸ“Š Statistiques AVANT standardisation :")
print(X.describe())

# Standardisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convertir en DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=clustering_features, index=X.index)

print("\nâœ… Standardisation terminÃ©e avec StandardScaler")

# ============================================
# 4. VÃ‰RIFICATION
# ============================================
print("\n" + "="*60)
print("âœ… VÃ‰RIFICATION DE LA STANDARDISATION")
print("="*60)

print("\nğŸ“Š Statistiques APRÃˆS standardisation :")
print(X_scaled_df.describe())

# VÃ©rifications critiques
mean_max = X_scaled_df.mean().abs().max()
std_mean = X_scaled_df.std().mean()

print(f"\nğŸ” VÃ©rifications critiques :")
print(f"   â€¢ Moyenne maximale (doit Ãªtre ~0) : {mean_max:.6f}")
print(f"   â€¢ Ã‰cart-type moyen (doit Ãªtre ~1) : {std_mean:.6f}")

if mean_max < 0.0001 and 0.99 < std_mean < 1.01:
    print("   âœ… Standardisation validÃ©e !")
else:
    print("   âš ï¸ Valeurs inhabituelles dÃ©tectÃ©es, vÃ©rifier les donnÃ©es")

print("\nğŸ“‹ AperÃ§u des donnÃ©es standardisÃ©es (5 premiÃ¨res lignes) :")
print(X_scaled_df.head())

# ============================================
# 5. SAUVEGARDE
# ============================================
print("\n" + "="*60)
print("ğŸ’¾ SAUVEGARDE DES RÃ‰SULTATS")
print("="*60)

# CrÃ©er le dossier output si nÃ©cessaire
os.makedirs('models', exist_ok=True)

# Sauvegarder les donnÃ©es standardisÃ©es
output_scaled = "models/scaled_features_for_clustering.csv"
X_scaled_df.to_csv(output_scaled, index=False, sep=';')
print(f"âœ… DonnÃ©es standardisÃ©es sauvegardÃ©es : {output_scaled}")

# Sauvegarder le scaler
scaler_file = 'output/scaler.pkl'
joblib.dump(scaler, scaler_file)
print(f"âœ… Scaler sauvegardÃ© : {scaler_file}")

# Sauvegarder aussi les index pour retrouver les clients plus tard
index_file = "output/clustering_indices.csv"
pd.DataFrame({'ID': df.loc[X.index, 'ID']}).to_csv(index_file, index=False)
print(f"âœ… Indices des clients sauvegardÃ©s : {index_file}")

# ============================================
# 6. RÃ‰SUMÃ‰ FINAL
# ============================================
print("\n" + "="*60)
print("ğŸ“Š RÃ‰SUMÃ‰ FINAL")
print("="*60)

print(f"\nâœ… Nombre de clients aprÃ¨s preprocessing : {X_scaled_df.shape[0]}")
print(f"âœ… Nombre de features : {X_scaled_df.shape[1]}")
print(f"\nğŸ“‚ Fichiers gÃ©nÃ©rÃ©s :")
print(f"   1. {output_scaled}")
print(f"   2. {scaler_file}")
print(f"   3. {index_file}")

print("\n" + "="*60)
print("âœ… STANDARDISATION TERMINÃ‰E AVEC SUCCÃˆS !")
print("="*60)
print("\nğŸ¯ PROCHAINES Ã‰TAPES :")
print("   1. Lancer le script 04_pca.py pour la rÃ©duction de dimensionnalitÃ©")
print("   2. DÃ©terminer le nombre optimal de clusters (Elbow + Silhouette)")
print("   3. Appliquer K-Means")
print("   4. Analyser les profils clients")
print("\nğŸš€ Dataset prÃªt pour la modÃ©lisation !")